{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module Five Assignment: Cartpole Problem\n",
    "Review the code in this notebook and in the score_logger.py file in the *scores* folder (directory). Once you have reviewed the code, return to this notebook and select **Cell** and then **Run All** from the menu bar to run this code. The code takes several minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random  \n",
    "import gym  \n",
    "import numpy as np  \n",
    "from collections import deque  \n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense  \n",
    "from keras.optimizers import Adam  \n",
    "  \n",
    "  \n",
    "from scores.score_logger import ScoreLogger  \n",
    "  \n",
    "ENV_NAME = \"CartPole-v1\"  \n",
    "  \n",
    "GAMMA = 0.95  \n",
    "LEARNING_RATE = 0.001  \n",
    "  \n",
    "MEMORY_SIZE = 1000000  \n",
    "BATCH_SIZE = 20  \n",
    "  \n",
    "EXPLORATION_MAX = 1.0  \n",
    "EXPLORATION_MIN = 0.01  \n",
    "EXPLORATION_DECAY = 0.995  \n",
    "  \n",
    "  \n",
    "class DQNSolver:  \n",
    "  \n",
    "    def __init__(self, observation_space, action_space):  \n",
    "        self.exploration_rate = EXPLORATION_MAX  \n",
    "  \n",
    "        self.action_space = action_space  \n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)  \n",
    "  \n",
    "        self.model = Sequential()  \n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))  \n",
    "        self.model.add(Dense(24, activation=\"relu\"))  \n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))  \n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))  \n",
    "  \n",
    "    def remember(self, state, action, reward, next_state, done):  \n",
    "        self.memory.append((state, action, reward, next_state, done))  \n",
    "  \n",
    "    def act(self, state):  \n",
    "        if np.random.rand() < self.exploration_rate:  \n",
    "            return random.randrange(self.action_space)  \n",
    "        q_values = self.model.predict(state)  \n",
    "        return np.argmax(q_values[0])  \n",
    "  \n",
    "    def experience_replay(self):  \n",
    "        if len(self.memory) < BATCH_SIZE:  \n",
    "            return  \n",
    "        batch = random.sample(self.memory, BATCH_SIZE)  \n",
    "        for state, action, reward, state_next, terminal in batch:  \n",
    "            q_update = reward  \n",
    "            if not terminal:  \n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))  \n",
    "            q_values = self.model.predict(state)  \n",
    "            q_values[0][action] = q_update  \n",
    "            self.model.fit(state, q_values, verbose=0)  \n",
    "        self.exploration_rate *= EXPLORATION_DECAY  \n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)  \n",
    "  \n",
    "  \n",
    "def cartpole():  \n",
    "    env = gym.make(ENV_NAME)  \n",
    "    score_logger = ScoreLogger(ENV_NAME)  \n",
    "    observation_space = env.observation_space.shape[0]  \n",
    "    action_space = env.action_space.n  \n",
    "    dqn_solver = DQNSolver(observation_space, action_space)  \n",
    "    run = 0  \n",
    "    while True:  \n",
    "        run += 1  \n",
    "        state = env.reset()  \n",
    "        state = np.reshape(state, [1, observation_space])  \n",
    "        step = 0  \n",
    "        while True:  \n",
    "            step += 1  \n",
    "            #env.render()  \n",
    "            action = dqn_solver.act(state)  \n",
    "            state_next, reward, terminal, info = env.step(action)  \n",
    "            reward = reward if not terminal else -reward  \n",
    "            state_next = np.reshape(state_next, [1, observation_space])  \n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)  \n",
    "            state = state_next  \n",
    "            if terminal:  \n",
    "                print (\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))  \n",
    "                score_logger.add_score(step, run)  \n",
    "                break  \n",
    "            dqn_solver.experience_replay()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 1, exploration: 1.0, score: 13\n",
      "Scores: (min: 13, avg: 13, max: 13)\n",
      "\n",
      "Run: 2, exploration: 0.9703725093562657, score: 13\n",
      "Scores: (min: 13, avg: 13, max: 13)\n",
      "\n",
      "Run: 3, exploration: 0.8224322824348486, score: 34\n",
      "Scores: (min: 13, avg: 20, max: 34)\n",
      "\n",
      "Run: 4, exploration: 0.7255664080186093, score: 26\n",
      "Scores: (min: 13, avg: 21.5, max: 34)\n",
      "\n",
      "Run: 5, exploration: 0.6662995813682115, score: 18\n",
      "Scores: (min: 13, avg: 20.8, max: 34)\n",
      "\n",
      "Run: 6, exploration: 0.567555222460375, score: 33\n",
      "Scores: (min: 13, avg: 22.833333333333332, max: 34)\n",
      "\n",
      "Run: 7, exploration: 0.4932355662165453, score: 29\n",
      "Scores: (min: 13, avg: 23.714285714285715, max: 34)\n",
      "\n",
      "Run: 8, exploration: 0.43732904629000013, score: 25\n",
      "Scores: (min: 13, avg: 23.875, max: 34)\n",
      "\n",
      "Run: 9, exploration: 0.3858205374665315, score: 26\n",
      "Scores: (min: 13, avg: 24.11111111111111, max: 34)\n",
      "\n",
      "Run: 10, exploration: 0.33698341088258443, score: 28\n",
      "Scores: (min: 13, avg: 24.5, max: 34)\n",
      "\n",
      "Run: 11, exploration: 0.28704309604425327, score: 33\n",
      "Scores: (min: 13, avg: 25.272727272727273, max: 34)\n",
      "\n",
      "Run: 12, exploration: 0.25450776877280124, score: 25\n",
      "Scores: (min: 13, avg: 25.25, max: 34)\n",
      "\n",
      "Run: 13, exploration: 0.22793384675362674, score: 23\n",
      "Scores: (min: 13, avg: 25.076923076923077, max: 34)\n",
      "\n",
      "Run: 14, exploration: 0.19908200231898848, score: 28\n",
      "Scores: (min: 13, avg: 25.285714285714285, max: 34)\n",
      "\n",
      "Run: 15, exploration: 0.16873052768933355, score: 34\n",
      "Scores: (min: 13, avg: 25.866666666666667, max: 34)\n",
      "\n",
      "Run: 16, exploration: 0.14590259167570602, score: 30\n",
      "Scores: (min: 13, avg: 26.125, max: 34)\n",
      "\n",
      "Run: 17, exploration: 0.11412846151764894, score: 50\n",
      "Scores: (min: 13, avg: 27.529411764705884, max: 50)\n",
      "\n",
      "Run: 18, exploration: 0.10119240105273684, score: 25\n",
      "Scores: (min: 13, avg: 27.38888888888889, max: 50)\n",
      "\n",
      "Run: 19, exploration: 0.07836551983717477, score: 52\n",
      "Scores: (min: 13, avg: 28.68421052631579, max: 52)\n",
      "\n",
      "Run: 20, exploration: 0.04537752660798141, score: 110\n",
      "Scores: (min: 13, avg: 32.75, max: 110)\n",
      "\n",
      "Run: 21, exploration: 0.024129537298457977, score: 127\n",
      "Scores: (min: 13, avg: 37.23809523809524, max: 127)\n",
      "\n",
      "Run: 22, exploration: 0.017951913959130504, score: 60\n",
      "Scores: (min: 13, avg: 38.27272727272727, max: 127)\n",
      "\n",
      "Run: 23, exploration: 0.012142582314594924, score: 79\n",
      "Scores: (min: 13, avg: 40.04347826086956, max: 127)\n",
      "\n",
      "Run: 24, exploration: 0.01, score: 88\n",
      "Scores: (min: 13, avg: 42.041666666666664, max: 127)\n",
      "\n",
      "Run: 25, exploration: 0.01, score: 133\n",
      "Scores: (min: 13, avg: 45.68, max: 133)\n",
      "\n",
      "Run: 26, exploration: 0.01, score: 132\n",
      "Scores: (min: 13, avg: 49, max: 133)\n",
      "\n",
      "Run: 27, exploration: 0.01, score: 105\n",
      "Scores: (min: 13, avg: 51.074074074074076, max: 133)\n",
      "\n",
      "Run: 28, exploration: 0.01, score: 107\n",
      "Scores: (min: 13, avg: 53.07142857142857, max: 133)\n",
      "\n",
      "Run: 29, exploration: 0.01, score: 155\n",
      "Scores: (min: 13, avg: 56.58620689655172, max: 155)\n",
      "\n",
      "Run: 30, exploration: 0.01, score: 165\n",
      "Scores: (min: 13, avg: 60.2, max: 165)\n",
      "\n",
      "Run: 31, exploration: 0.01, score: 241\n",
      "Scores: (min: 13, avg: 66.03225806451613, max: 241)\n",
      "\n",
      "Run: 32, exploration: 0.01, score: 310\n",
      "Scores: (min: 13, avg: 73.65625, max: 310)\n",
      "\n",
      "Run: 33, exploration: 0.01, score: 182\n",
      "Scores: (min: 13, avg: 76.93939393939394, max: 310)\n",
      "\n",
      "Run: 34, exploration: 0.01, score: 192\n",
      "Scores: (min: 13, avg: 80.32352941176471, max: 310)\n",
      "\n",
      "Run: 35, exploration: 0.01, score: 166\n",
      "Scores: (min: 13, avg: 82.77142857142857, max: 310)\n",
      "\n",
      "Run: 36, exploration: 0.01, score: 201\n",
      "Scores: (min: 13, avg: 86.05555555555556, max: 310)\n",
      "\n",
      "Run: 37, exploration: 0.01, score: 153\n",
      "Scores: (min: 13, avg: 87.86486486486487, max: 310)\n",
      "\n",
      "Run: 38, exploration: 0.01, score: 140\n",
      "Scores: (min: 13, avg: 89.23684210526316, max: 310)\n",
      "\n",
      "Run: 39, exploration: 0.01, score: 169\n",
      "Scores: (min: 13, avg: 91.28205128205128, max: 310)\n",
      "\n",
      "Run: 40, exploration: 0.01, score: 166\n",
      "Scores: (min: 13, avg: 93.15, max: 310)\n",
      "\n",
      "Run: 41, exploration: 0.01, score: 139\n",
      "Scores: (min: 13, avg: 94.26829268292683, max: 310)\n",
      "\n",
      "Run: 42, exploration: 0.01, score: 220\n",
      "Scores: (min: 13, avg: 97.26190476190476, max: 310)\n",
      "\n",
      "Run: 43, exploration: 0.01, score: 131\n",
      "Scores: (min: 13, avg: 98.04651162790698, max: 310)\n",
      "\n",
      "Run: 44, exploration: 0.01, score: 165\n",
      "Scores: (min: 13, avg: 99.56818181818181, max: 310)\n",
      "\n",
      "Run: 45, exploration: 0.01, score: 163\n",
      "Scores: (min: 13, avg: 100.97777777777777, max: 310)\n",
      "\n",
      "Run: 46, exploration: 0.01, score: 133\n",
      "Scores: (min: 13, avg: 101.67391304347827, max: 310)\n",
      "\n",
      "Run: 47, exploration: 0.01, score: 191\n",
      "Scores: (min: 13, avg: 103.57446808510639, max: 310)\n",
      "\n",
      "Run: 48, exploration: 0.01, score: 166\n",
      "Scores: (min: 13, avg: 104.875, max: 310)\n",
      "\n",
      "Run: 49, exploration: 0.01, score: 165\n",
      "Scores: (min: 13, avg: 106.10204081632654, max: 310)\n",
      "\n",
      "Run: 50, exploration: 0.01, score: 196\n",
      "Scores: (min: 13, avg: 107.9, max: 310)\n",
      "\n",
      "Run: 51, exploration: 0.01, score: 172\n",
      "Scores: (min: 13, avg: 109.15686274509804, max: 310)\n",
      "\n",
      "Run: 52, exploration: 0.01, score: 166\n",
      "Scores: (min: 13, avg: 110.25, max: 310)\n",
      "\n",
      "Run: 53, exploration: 0.01, score: 169\n",
      "Scores: (min: 13, avg: 111.35849056603773, max: 310)\n",
      "\n",
      "Run: 54, exploration: 0.01, score: 146\n",
      "Scores: (min: 13, avg: 112, max: 310)\n",
      "\n",
      "Run: 55, exploration: 0.01, score: 194\n",
      "Scores: (min: 13, avg: 113.49090909090908, max: 310)\n",
      "\n",
      "Run: 56, exploration: 0.01, score: 169\n",
      "Scores: (min: 13, avg: 114.48214285714286, max: 310)\n",
      "\n",
      "Run: 57, exploration: 0.01, score: 148\n",
      "Scores: (min: 13, avg: 115.0701754385965, max: 310)\n",
      "\n",
      "Run: 58, exploration: 0.01, score: 278\n",
      "Scores: (min: 13, avg: 117.87931034482759, max: 310)\n",
      "\n",
      "Run: 59, exploration: 0.01, score: 120\n",
      "Scores: (min: 13, avg: 117.91525423728814, max: 310)\n",
      "\n",
      "Run: 60, exploration: 0.01, score: 212\n",
      "Scores: (min: 13, avg: 119.48333333333333, max: 310)\n",
      "\n",
      "Run: 61, exploration: 0.01, score: 147\n",
      "Scores: (min: 13, avg: 119.93442622950819, max: 310)\n",
      "\n",
      "Run: 62, exploration: 0.01, score: 203\n",
      "Scores: (min: 13, avg: 121.2741935483871, max: 310)\n",
      "\n",
      "Run: 63, exploration: 0.01, score: 48\n",
      "Scores: (min: 13, avg: 120.11111111111111, max: 310)\n",
      "\n",
      "Run: 64, exploration: 0.01, score: 174\n",
      "Scores: (min: 13, avg: 120.953125, max: 310)\n",
      "\n",
      "Run: 65, exploration: 0.01, score: 134\n",
      "Scores: (min: 13, avg: 121.15384615384616, max: 310)\n",
      "\n",
      "Run: 66, exploration: 0.01, score: 149\n",
      "Scores: (min: 13, avg: 121.57575757575758, max: 310)\n",
      "\n",
      "Run: 67, exploration: 0.01, score: 158\n",
      "Scores: (min: 13, avg: 122.11940298507463, max: 310)\n",
      "\n",
      "Run: 68, exploration: 0.01, score: 165\n",
      "Scores: (min: 13, avg: 122.75, max: 310)\n",
      "\n",
      "Run: 69, exploration: 0.01, score: 236\n",
      "Scores: (min: 13, avg: 124.3913043478261, max: 310)\n",
      "\n",
      "Run: 70, exploration: 0.01, score: 182\n",
      "Scores: (min: 13, avg: 125.21428571428571, max: 310)\n",
      "\n",
      "Run: 71, exploration: 0.01, score: 185\n",
      "Scores: (min: 13, avg: 126.05633802816901, max: 310)\n",
      "\n",
      "Run: 72, exploration: 0.01, score: 170\n",
      "Scores: (min: 13, avg: 126.66666666666667, max: 310)\n",
      "\n",
      "Run: 73, exploration: 0.01, score: 328\n",
      "Scores: (min: 13, avg: 129.42465753424656, max: 328)\n",
      "\n",
      "Run: 74, exploration: 0.01, score: 231\n",
      "Scores: (min: 13, avg: 130.7972972972973, max: 328)\n",
      "\n",
      "Run: 75, exploration: 0.01, score: 197\n",
      "Scores: (min: 13, avg: 131.68, max: 328)\n",
      "\n",
      "Run: 76, exploration: 0.01, score: 124\n",
      "Scores: (min: 13, avg: 131.57894736842104, max: 328)\n",
      "\n",
      "Run: 77, exploration: 0.01, score: 132\n",
      "Scores: (min: 13, avg: 131.58441558441558, max: 328)\n",
      "\n",
      "Run: 78, exploration: 0.01, score: 149\n",
      "Scores: (min: 13, avg: 131.80769230769232, max: 328)\n",
      "\n",
      "Run: 79, exploration: 0.01, score: 52\n",
      "Scores: (min: 13, avg: 130.79746835443038, max: 328)\n",
      "\n",
      "Run: 80, exploration: 0.01, score: 191\n",
      "Scores: (min: 13, avg: 131.55, max: 328)\n",
      "\n",
      "Run: 81, exploration: 0.01, score: 180\n",
      "Scores: (min: 13, avg: 132.14814814814815, max: 328)\n",
      "\n",
      "Run: 82, exploration: 0.01, score: 288\n",
      "Scores: (min: 13, avg: 134.0487804878049, max: 328)\n",
      "\n",
      "Run: 83, exploration: 0.01, score: 204\n",
      "Scores: (min: 13, avg: 134.89156626506025, max: 328)\n",
      "\n",
      "Run: 84, exploration: 0.01, score: 57\n",
      "Scores: (min: 13, avg: 133.96428571428572, max: 328)\n",
      "\n",
      "Run: 85, exploration: 0.01, score: 127\n",
      "Scores: (min: 13, avg: 133.88235294117646, max: 328)\n",
      "\n",
      "Run: 86, exploration: 0.01, score: 162\n",
      "Scores: (min: 13, avg: 134.2093023255814, max: 328)\n",
      "\n",
      "Run: 87, exploration: 0.01, score: 143\n",
      "Scores: (min: 13, avg: 134.31034482758622, max: 328)\n",
      "\n",
      "Run: 88, exploration: 0.01, score: 191\n",
      "Scores: (min: 13, avg: 134.95454545454547, max: 328)\n",
      "\n",
      "Run: 89, exploration: 0.01, score: 164\n",
      "Scores: (min: 13, avg: 135.2808988764045, max: 328)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 90, exploration: 0.01, score: 170\n",
      "Scores: (min: 13, avg: 135.66666666666666, max: 328)\n",
      "\n",
      "Run: 91, exploration: 0.01, score: 120\n",
      "Scores: (min: 13, avg: 135.4945054945055, max: 328)\n",
      "\n",
      "Run: 92, exploration: 0.01, score: 118\n",
      "Scores: (min: 13, avg: 135.30434782608697, max: 328)\n",
      "\n",
      "Run: 93, exploration: 0.01, score: 38\n",
      "Scores: (min: 13, avg: 134.25806451612902, max: 328)\n",
      "\n",
      "Run: 94, exploration: 0.01, score: 170\n",
      "Scores: (min: 13, avg: 134.63829787234042, max: 328)\n",
      "\n",
      "Run: 95, exploration: 0.01, score: 241\n",
      "Scores: (min: 13, avg: 135.7578947368421, max: 328)\n",
      "\n",
      "Run: 96, exploration: 0.01, score: 193\n",
      "Scores: (min: 13, avg: 136.35416666666666, max: 328)\n",
      "\n",
      "Run: 97, exploration: 0.01, score: 150\n",
      "Scores: (min: 13, avg: 136.49484536082474, max: 328)\n",
      "\n",
      "Run: 98, exploration: 0.01, score: 176\n",
      "Scores: (min: 13, avg: 136.89795918367346, max: 328)\n",
      "\n",
      "Run: 99, exploration: 0.01, score: 219\n",
      "Scores: (min: 13, avg: 137.72727272727272, max: 328)\n",
      "\n",
      "Run: 100, exploration: 0.01, score: 157\n",
      "Scores: (min: 13, avg: 137.92, max: 328)\n",
      "\n",
      "Run: 101, exploration: 0.01, score: 198\n",
      "Scores: (min: 13, avg: 139.77, max: 328)\n",
      "\n",
      "Run: 102, exploration: 0.01, score: 120\n",
      "Scores: (min: 18, avg: 140.84, max: 328)\n",
      "\n",
      "Run: 103, exploration: 0.01, score: 153\n",
      "Scores: (min: 18, avg: 142.03, max: 328)\n",
      "\n",
      "Run: 104, exploration: 0.01, score: 151\n",
      "Scores: (min: 18, avg: 143.28, max: 328)\n",
      "\n",
      "Run: 105, exploration: 0.01, score: 212\n",
      "Scores: (min: 23, avg: 145.22, max: 328)\n",
      "\n",
      "Run: 106, exploration: 0.01, score: 200\n",
      "Scores: (min: 23, avg: 146.89, max: 328)\n",
      "\n",
      "Run: 107, exploration: 0.01, score: 197\n",
      "Scores: (min: 23, avg: 148.57, max: 328)\n",
      "\n",
      "Run: 108, exploration: 0.01, score: 35\n",
      "Scores: (min: 23, avg: 148.67, max: 328)\n",
      "\n",
      "Run: 109, exploration: 0.01, score: 206\n",
      "Scores: (min: 23, avg: 150.47, max: 328)\n",
      "\n",
      "Run: 110, exploration: 0.01, score: 129\n",
      "Scores: (min: 23, avg: 151.48, max: 328)\n",
      "\n",
      "Run: 111, exploration: 0.01, score: 27\n",
      "Scores: (min: 23, avg: 151.42, max: 328)\n",
      "\n",
      "Run: 112, exploration: 0.01, score: 58\n",
      "Scores: (min: 23, avg: 151.75, max: 328)\n",
      "\n",
      "Run: 113, exploration: 0.01, score: 151\n",
      "Scores: (min: 25, avg: 153.03, max: 328)\n",
      "\n",
      "Run: 114, exploration: 0.01, score: 157\n",
      "Scores: (min: 25, avg: 154.32, max: 328)\n",
      "\n",
      "Run: 115, exploration: 0.01, score: 168\n",
      "Scores: (min: 25, avg: 155.66, max: 328)\n",
      "\n",
      "Run: 116, exploration: 0.01, score: 222\n",
      "Scores: (min: 25, avg: 157.58, max: 328)\n",
      "\n",
      "Run: 117, exploration: 0.01, score: 110\n",
      "Scores: (min: 25, avg: 158.18, max: 328)\n",
      "\n",
      "Run: 118, exploration: 0.01, score: 184\n",
      "Scores: (min: 27, avg: 159.77, max: 328)\n",
      "\n",
      "Run: 119, exploration: 0.01, score: 195\n",
      "Scores: (min: 27, avg: 161.2, max: 328)\n",
      "\n",
      "Run: 120, exploration: 0.01, score: 244\n",
      "Scores: (min: 27, avg: 162.54, max: 328)\n",
      "\n",
      "Run: 121, exploration: 0.01, score: 142\n",
      "Scores: (min: 27, avg: 162.69, max: 328)\n",
      "\n",
      "Run: 122, exploration: 0.01, score: 170\n",
      "Scores: (min: 27, avg: 163.79, max: 328)\n",
      "\n",
      "Run: 123, exploration: 0.01, score: 143\n",
      "Scores: (min: 27, avg: 164.43, max: 328)\n",
      "\n",
      "Run: 124, exploration: 0.01, score: 192\n",
      "Scores: (min: 27, avg: 165.47, max: 328)\n",
      "\n",
      "Run: 125, exploration: 0.01, score: 232\n",
      "Scores: (min: 27, avg: 166.46, max: 328)\n",
      "\n",
      "Run: 126, exploration: 0.01, score: 146\n",
      "Scores: (min: 27, avg: 166.6, max: 328)\n",
      "\n",
      "Run: 127, exploration: 0.01, score: 216\n",
      "Scores: (min: 27, avg: 167.71, max: 328)\n",
      "\n",
      "Run: 128, exploration: 0.01, score: 132\n",
      "Scores: (min: 27, avg: 167.96, max: 328)\n",
      "\n",
      "Run: 129, exploration: 0.01, score: 216\n",
      "Scores: (min: 27, avg: 168.57, max: 328)\n",
      "\n",
      "Run: 130, exploration: 0.01, score: 234\n",
      "Scores: (min: 27, avg: 169.26, max: 328)\n",
      "\n",
      "Run: 131, exploration: 0.01, score: 37\n",
      "Scores: (min: 27, avg: 167.22, max: 328)\n",
      "\n",
      "Run: 132, exploration: 0.01, score: 19\n",
      "Scores: (min: 19, avg: 164.31, max: 328)\n",
      "\n",
      "Run: 133, exploration: 0.01, score: 175\n",
      "Scores: (min: 19, avg: 164.24, max: 328)\n",
      "\n",
      "Run: 134, exploration: 0.01, score: 165\n",
      "Scores: (min: 19, avg: 163.97, max: 328)\n",
      "\n",
      "Run: 135, exploration: 0.01, score: 51\n",
      "Scores: (min: 19, avg: 162.82, max: 328)\n",
      "\n",
      "Run: 136, exploration: 0.01, score: 155\n",
      "Scores: (min: 19, avg: 162.36, max: 328)\n",
      "\n",
      "Run: 137, exploration: 0.01, score: 225\n",
      "Scores: (min: 19, avg: 163.08, max: 328)\n",
      "\n",
      "Run: 138, exploration: 0.01, score: 295\n",
      "Scores: (min: 19, avg: 164.63, max: 328)\n",
      "\n",
      "Run: 139, exploration: 0.01, score: 338\n",
      "Scores: (min: 19, avg: 166.32, max: 338)\n",
      "\n",
      "Run: 140, exploration: 0.01, score: 243\n",
      "Scores: (min: 19, avg: 167.09, max: 338)\n",
      "\n",
      "Run: 141, exploration: 0.01, score: 240\n",
      "Scores: (min: 19, avg: 168.1, max: 338)\n",
      "\n",
      "Run: 142, exploration: 0.01, score: 148\n",
      "Scores: (min: 19, avg: 167.38, max: 338)\n",
      "\n",
      "Run: 143, exploration: 0.01, score: 245\n",
      "Scores: (min: 19, avg: 168.52, max: 338)\n",
      "\n",
      "Run: 144, exploration: 0.01, score: 245\n",
      "Scores: (min: 19, avg: 169.32, max: 338)\n",
      "\n",
      "Run: 145, exploration: 0.01, score: 242\n",
      "Scores: (min: 19, avg: 170.11, max: 338)\n",
      "\n",
      "Run: 146, exploration: 0.01, score: 254\n",
      "Scores: (min: 19, avg: 171.32, max: 338)\n",
      "\n",
      "Run: 147, exploration: 0.01, score: 193\n",
      "Scores: (min: 19, avg: 171.34, max: 338)\n",
      "\n",
      "Run: 148, exploration: 0.01, score: 204\n",
      "Scores: (min: 19, avg: 171.72, max: 338)\n",
      "\n",
      "Run: 149, exploration: 0.01, score: 162\n",
      "Scores: (min: 19, avg: 171.69, max: 338)\n",
      "\n",
      "Run: 150, exploration: 0.01, score: 242\n",
      "Scores: (min: 19, avg: 172.15, max: 338)\n",
      "\n",
      "Run: 151, exploration: 0.01, score: 154\n",
      "Scores: (min: 19, avg: 171.97, max: 338)\n",
      "\n",
      "Run: 152, exploration: 0.01, score: 164\n",
      "Scores: (min: 19, avg: 171.95, max: 338)\n",
      "\n",
      "Run: 153, exploration: 0.01, score: 198\n",
      "Scores: (min: 19, avg: 172.24, max: 338)\n",
      "\n",
      "Run: 154, exploration: 0.01, score: 288\n",
      "Scores: (min: 19, avg: 173.66, max: 338)\n",
      "\n",
      "Run: 155, exploration: 0.01, score: 127\n",
      "Scores: (min: 19, avg: 172.99, max: 338)\n",
      "\n",
      "Run: 156, exploration: 0.01, score: 113\n",
      "Scores: (min: 19, avg: 172.43, max: 338)\n",
      "\n",
      "Run: 157, exploration: 0.01, score: 171\n",
      "Scores: (min: 19, avg: 172.66, max: 338)\n",
      "\n",
      "Run: 158, exploration: 0.01, score: 196\n",
      "Scores: (min: 19, avg: 171.84, max: 338)\n",
      "\n",
      "Run: 159, exploration: 0.01, score: 200\n",
      "Scores: (min: 19, avg: 172.64, max: 338)\n",
      "\n",
      "Run: 160, exploration: 0.01, score: 329\n",
      "Scores: (min: 19, avg: 173.81, max: 338)\n",
      "\n",
      "Run: 161, exploration: 0.01, score: 500\n",
      "Scores: (min: 19, avg: 177.34, max: 500)\n",
      "\n",
      "Run: 162, exploration: 0.01, score: 324\n",
      "Scores: (min: 19, avg: 178.55, max: 500)\n",
      "\n",
      "Run: 163, exploration: 0.01, score: 231\n",
      "Scores: (min: 19, avg: 180.38, max: 500)\n",
      "\n",
      "Run: 164, exploration: 0.01, score: 500\n",
      "Scores: (min: 19, avg: 183.64, max: 500)\n",
      "\n",
      "Run: 165, exploration: 0.01, score: 295\n",
      "Scores: (min: 19, avg: 185.25, max: 500)\n",
      "\n",
      "Run: 166, exploration: 0.01, score: 163\n",
      "Scores: (min: 19, avg: 185.39, max: 500)\n",
      "\n",
      "Run: 167, exploration: 0.01, score: 156\n",
      "Scores: (min: 19, avg: 185.37, max: 500)\n",
      "\n",
      "Run: 168, exploration: 0.01, score: 113\n",
      "Scores: (min: 19, avg: 184.85, max: 500)\n",
      "\n",
      "Run: 169, exploration: 0.01, score: 233\n",
      "Scores: (min: 19, avg: 184.82, max: 500)\n",
      "\n",
      "Run: 170, exploration: 0.01, score: 500\n",
      "Scores: (min: 19, avg: 188, max: 500)\n",
      "\n",
      "Run: 171, exploration: 0.01, score: 245\n",
      "Scores: (min: 19, avg: 188.6, max: 500)\n",
      "\n",
      "Run: 172, exploration: 0.01, score: 149\n",
      "Scores: (min: 19, avg: 188.39, max: 500)\n",
      "\n",
      "Run: 173, exploration: 0.01, score: 12\n",
      "Scores: (min: 12, avg: 185.23, max: 500)\n",
      "\n",
      "Run: 174, exploration: 0.01, score: 15\n",
      "Scores: (min: 12, avg: 183.07, max: 500)\n",
      "\n",
      "Run: 175, exploration: 0.01, score: 16\n",
      "Scores: (min: 12, avg: 181.26, max: 500)\n",
      "\n",
      "Run: 176, exploration: 0.01, score: 69\n",
      "Scores: (min: 12, avg: 180.71, max: 500)\n",
      "\n",
      "Run: 177, exploration: 0.01, score: 133\n",
      "Scores: (min: 12, avg: 180.72, max: 500)\n",
      "\n",
      "Run: 178, exploration: 0.01, score: 134\n",
      "Scores: (min: 12, avg: 180.57, max: 500)\n",
      "\n",
      "Run: 179, exploration: 0.01, score: 177\n",
      "Scores: (min: 12, avg: 181.82, max: 500)\n",
      "\n",
      "Run: 180, exploration: 0.01, score: 477\n",
      "Scores: (min: 12, avg: 184.68, max: 500)\n",
      "\n",
      "Run: 181, exploration: 0.01, score: 324\n",
      "Scores: (min: 12, avg: 186.12, max: 500)\n",
      "\n",
      "Run: 182, exploration: 0.01, score: 235\n",
      "Scores: (min: 12, avg: 185.59, max: 500)\n",
      "\n",
      "Run: 183, exploration: 0.01, score: 274\n",
      "Scores: (min: 12, avg: 186.29, max: 500)\n",
      "\n",
      "Run: 184, exploration: 0.01, score: 280\n",
      "Scores: (min: 12, avg: 188.52, max: 500)\n",
      "\n",
      "Run: 185, exploration: 0.01, score: 18\n",
      "Scores: (min: 12, avg: 187.43, max: 500)\n",
      "\n",
      "Run: 186, exploration: 0.01, score: 13\n",
      "Scores: (min: 12, avg: 185.94, max: 500)\n",
      "\n",
      "Run: 187, exploration: 0.01, score: 155\n",
      "Scores: (min: 12, avg: 186.06, max: 500)\n",
      "\n",
      "Run: 188, exploration: 0.01, score: 500\n",
      "Scores: (min: 12, avg: 189.15, max: 500)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run: 189, exploration: 0.01, score: 133\n",
      "Scores: (min: 12, avg: 188.84, max: 500)\n",
      "\n",
      "Run: 190, exploration: 0.01, score: 120\n",
      "Scores: (min: 12, avg: 188.34, max: 500)\n",
      "\n",
      "Run: 191, exploration: 0.01, score: 276\n",
      "Scores: (min: 12, avg: 189.9, max: 500)\n",
      "\n",
      "Run: 192, exploration: 0.01, score: 192\n",
      "Scores: (min: 12, avg: 190.64, max: 500)\n",
      "\n",
      "Run: 193, exploration: 0.01, score: 388\n",
      "Scores: (min: 12, avg: 194.14, max: 500)\n",
      "\n",
      "Run: 194, exploration: 0.01, score: 192\n",
      "Scores: (min: 12, avg: 194.36, max: 500)\n",
      "\n",
      "Run: 195, exploration: 0.01, score: 351\n",
      "Scores: (min: 12, avg: 195.46, max: 500)\n",
      "\n",
      "Solved in 95 runs, 195 total runs.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e23913c48dc8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcartpole\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-bea6327a4fd8>\u001b[0m in \u001b[0;36mcartpole\u001b[1;34m()\u001b[0m\n\u001b[0;32m     84\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mterminal\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"Run: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\", exploration: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdqn_solver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexploration_rate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\", score: \"\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 86\u001b[1;33m                 \u001b[0mscore_logger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     87\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m             \u001b[0mdqn_solver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperience_replay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mU:\\scores\\score_logger.py\u001b[0m in \u001b[0;36madd_score\u001b[1;34m(self, score, run)\u001b[0m\n\u001b[0;32m     52\u001b[0m             \u001b[1;31m#               show_trend=False,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[1;31m#               show_legend=False)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[0mexit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_save_png\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage_of_n_last\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_goal\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_trend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_legend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "cartpole()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: If the code is running properly, you should begin to see output appearing above this code block. It will take several minutes, so it is recommended that you let this code run in the background while completing other work. When the code has finished, it will print output saying, \"Solved in _ runs, _ total runs.\"\n",
    "\n",
    "You may see an error about not having an exit command. This error does not affect the program's functionality and results from the steps taken to convert the code from Python 2.x to Python 3. Please disregard this error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing Learning Rate\n",
    "import random  \n",
    "import gym  \n",
    "import numpy as np  \n",
    "from collections import deque  \n",
    "from keras.models import Sequential  \n",
    "from keras.layers import Dense  \n",
    "from keras.optimizers import Adam  \n",
    "  \n",
    "  \n",
    "from scores.score_logger import ScoreLogger  \n",
    "  \n",
    "ENV_NAME = \"CartPole-v1\"  \n",
    "  \n",
    "GAMMA = 0.95  \n",
    "LEARNING_RATE = 0.005  \n",
    "  \n",
    "MEMORY_SIZE = 1000000  \n",
    "BATCH_SIZE = 20  \n",
    "  \n",
    "EXPLORATION_MAX = 1.0  \n",
    "EXPLORATION_MIN = 0.01  \n",
    "EXPLORATION_DECAY = 0.995  \n",
    "  \n",
    "  \n",
    "class DQNSolver:  \n",
    "  \n",
    "    def __init__(self, observation_space, action_space):  \n",
    "        self.exploration_rate = EXPLORATION_MAX  \n",
    "  \n",
    "        self.action_space = action_space  \n",
    "        self.memory = deque(maxlen=MEMORY_SIZE)  \n",
    "  \n",
    "        self.model = Sequential()  \n",
    "        self.model.add(Dense(24, input_shape=(observation_space,), activation=\"relu\"))  \n",
    "        self.model.add(Dense(24, activation=\"relu\"))  \n",
    "        self.model.add(Dense(self.action_space, activation=\"linear\"))  \n",
    "        self.model.compile(loss=\"mse\", optimizer=Adam(lr=LEARNING_RATE))  \n",
    "  \n",
    "    def remember(self, state, action, reward, next_state, done):  \n",
    "        self.memory.append((state, action, reward, next_state, done))  \n",
    "  \n",
    "    def act(self, state):  \n",
    "        if np.random.rand() < self.exploration_rate:  \n",
    "            return random.randrange(self.action_space)  \n",
    "        q_values = self.model.predict(state)  \n",
    "        return np.argmax(q_values[0])  \n",
    "  \n",
    "    def experience_replay(self):  \n",
    "        if len(self.memory) < BATCH_SIZE:  \n",
    "            return  \n",
    "        batch = random.sample(self.memory, BATCH_SIZE)  \n",
    "        for state, action, reward, state_next, terminal in batch:  \n",
    "            q_update = reward  \n",
    "            if not terminal:  \n",
    "                q_update = (reward + GAMMA * np.amax(self.model.predict(state_next)[0]))  \n",
    "            q_values = self.model.predict(state)  \n",
    "            q_values[0][action] = q_update  \n",
    "            self.model.fit(state, q_values, verbose=0)  \n",
    "        self.exploration_rate *= EXPLORATION_DECAY  \n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)  \n",
    "  \n",
    "  \n",
    "def cartpole():  \n",
    "    env = gym.make(ENV_NAME)  \n",
    "    score_logger = ScoreLogger(ENV_NAME)  \n",
    "    observation_space = env.observation_space.shape[0]  \n",
    "    action_space = env.action_space.n  \n",
    "    dqn_solver = DQNSolver(observation_space, action_space)  \n",
    "    run = 0  \n",
    "    while True:  \n",
    "        run += 1  \n",
    "        state = env.reset()  \n",
    "        state = np.reshape(state, [1, observation_space])  \n",
    "        step = 0  \n",
    "        while True:  \n",
    "            step += 1  \n",
    "            #env.render()  \n",
    "            action = dqn_solver.act(state)  \n",
    "            state_next, reward, terminal, info = env.step(action)  \n",
    "            reward = reward if not terminal else -reward  \n",
    "            state_next = np.reshape(state_next, [1, observation_space])  \n",
    "            dqn_solver.remember(state, action, reward, state_next, terminal)  \n",
    "            state = state_next  \n",
    "            if terminal:  \n",
    "                print (\"Run: \" + str(run) + \", exploration: \" + str(dqn_solver.exploration_rate) + \", score: \" + str(step))  \n",
    "                score_logger.add_score(step, run)  \n",
    "                break  \n",
    "            dqn_solver.experience_replay()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Explain how reinforcement learning concepts apply to the cartpole problem.\n",
    "    What is the goal of the agent in this case?\n",
    "        The goal of the agent in this assignment was to keep the pole balanced on the cart for as long as they can.\n",
    "        \n",
    "    What are the various state values?\n",
    "        The different states that I can think of would be the pole staying stationary, the pole not falling over, and the pole falling over\n",
    "        \n",
    "    What are the possible actions that can be performed?\n",
    "        I think the actions that could happen would be moving the cart either in the left or right direction\n",
    "        \n",
    "    What reinforcement algorithm is used for this problem?\n",
    "        I believe that the algorithm that is being used is Q-learning as the AI is trained on a pass or fail if it makes the right move and either has the pole staying upright or making the incorrect move that would result in the pole falling over.\n",
    "        \n",
    "Analyze how experience replay is applied to the cartpole problem.\n",
    "    How does experience replay work in this algorithm?\n",
    "        With this type of learning model, it requires it to store and think on the different things that are happening such as the state, the action perfromed, the reward or punishment, and the next state as well as the random sampling.\n",
    "        \n",
    "    What is the effect of introducing a discount factor for calculating the future rewards?\n",
    "        The overall importance of the discount factor is to try and make the AI perform better faster by making the rewards that it is given change depend on the amount of time that has passed.\n",
    "        \n",
    "Analyze how neural networks are used in deep Q-learning.\n",
    "    Explain the neural network architecture that is used in the cartpole problem.\n",
    "        I think that this problem uses the Deep Q-learning,  I mainly think this instead of a traditional Q learning is that it ditches the Q table and adds in a neural network that will produce more than one Q value. \n",
    "       \n",
    "    How does the neural network make the Q-learning algorithm more efficient?\n",
    "        Why this was done is because Q learning is less efficient and scalable than deep Q learning which will provide different Q values and then selects the action to be taken based off which one is the highest. It also ditches the Q table entierly and the updates that would happen when using just Q learning.\n",
    "        \n",
    "    What difference do you see in the algorithm performance when you increase or decrease the learning rate?\n",
    "        What I would see when Increasing the learning rate was it would take less time to get the AI to be better at the game and the opposite seems to be true if I lower the learning rate it seems to make the training process take longer.\n",
    "        \n",
    "Referneces:\n",
    "GfG. (2023, January 23). Deep Q-learning. GeeksforGeeks. https://www.geeksforgeeks.org/deep-q-learning/# \n",
    "Lamba, A. (2018, September 3). An introduction to Q-learning: Reinforcement learning. Medium. https://medium.com/free-code-camp/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc \n",
    "Loeber, P. (2022, February 10). Reinforcement learning with (deep) Q-learning explained. News, Tutorials, AI Research. https://www.assemblyai.com/blog/reinforcement-learning-with-deep-q-learning-explained/ \n",
    "Singh, S. (2022, October 7). A comprehensive guide to neural networks in Deep Q-Learning. A Comprehensive Guide to Neural Networks in Deep Q-learning. https://www.turing.com/kb/how-are-neural-networks-used-in-deep-q-learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
